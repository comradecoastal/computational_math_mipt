{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3iN94FgizQU"
   },
   "source": [
    "Лекция 9:\n",
    "* Сингулярное разложение\n",
    "* Простой итерационный алгоритм сингулярного разложения \n",
    "* Метод Якоби"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DwEX_GXEjrx"
   },
   "source": [
    "# Сингулярное разложение: \n",
    "\n",
    "Теорема : \n",
    "Пусть дана матрица $A$ размера $m$ на $n$. Тогда существуют такие ортнормированные базисы $e^k$  и $q^l$ и положительные числа $\\sigma_1 \\geq \\sigma_2\\geq ...\\geq \\sigma_r$ и $0\\geq r \\geq min(n, m)$ такие, что:\n",
    "\n",
    "$Ae^k = \\begin{cases}\\sigma_k q^k, k\\leq r\\\\ 0, k > r \\end{cases}$.\n",
    "\n",
    "Числа $\\sigma$ называются сингулярными числами, а базисы называются сингуляными базисами. Тут $r$ - ранг матрицы $A$.\n",
    "\n",
    "Докажем :\n",
    "\n",
    "Матрица $A^* A$ - самосопряжена и неотрицательно определена. Все её собственные числа неотрицательны, и есть базис собственных векторов $e^k$. Тогда:\n",
    "\n",
    "$A*Ax =\\sigma_k^2 xe^k, k = 1, 2, ..., n$\n",
    "\n",
    "Будем считать, что все сигма упорядочены по убыванию. Положим $z^k =Ae^k$ и заметим: \n",
    "$(z^p, z^q) = (Ae^p, Ae^q) = \\sigma_p^2(e^p, e^q)$, откуда:\n",
    "\n",
    "$(z^p, z^q) = \\begin{cases}0, p\\neq q\\\\ \\sigma_p^2, p = q \\end{cases}$\n",
    "\n",
    "Следовательно векторы $q^k =\\sigma_p^{-1}Ae^k$ образуют ортнормированную систему в пространстве $\\mathbb{C}^m$. ЧТД.\n",
    "\n",
    "Рассмотрим в матричном виде:\n",
    "\n",
    "$M = U\\Sigma V^*$, где $\\Sigma$ - матрица размера $m$ на $n$, на диагонали которой лежат сингулярный числа, а все остальные элементы нулевые. Матрицы $U, V$ состоят из левых и правых сингулярных векторов соответсвенно.\n",
    "* Левые сингулярные вектора - собственные вектора матрицы $MM^*$\n",
    "* Правые сингулярные вектора - собственные вектора матрицы $M^*M$\n",
    "\n",
    "Применение сингулярного разложения:\n",
    "* Полярное разложение\n",
    "* Решение СЛАУ\n",
    "* Приближение матрицей меньшего ранга\n",
    "* Сокращение объема памяти изображений без существенных потерь качества "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EES4MyEtJYNg"
   },
   "source": [
    "# Простой итерационный алгоритм\n",
    "\n",
    "Основная процедура — поиск наилучшего приближения произвольной $ m \\times n$  матрицы $X=(x_{ij})$ матрицей вида $b \\otimes a = (b_i a_j)$ (где b — m-мерный вектор, а a — n-мерный вектор) методом наименьших квадратов:\n",
    "\n",
    "\n",
    "$F(b, a) = \\frac{1}{2}\\sum_{i=1}^m \\sum_{j=1}^n (x_{ij} - b_i a_j )^2 \\to \\min$\n",
    "\n",
    "\n",
    "Решение этой задачи дается последовательными итерациями по явным формулам. При фиксированном векторе $a=(a_j)$  значения $b=(b_i)$ , доставляющие минимум форме $F(b, a)$ , однозначно и явно определяются из равенств $\\partial F/ \\partial b_i = 0$ :\n",
    "\n",
    "$\\frac{\\partial F}{\\partial b_i}$ $= - \\sum_{j=1}^n (x_{ij} - b_i a_j )a_j = 0$; $b_i = \\frac{\\sum_{j=1}^n x_{ij}  a_j}{\\sum_{j=1}^n a_j^2 } $. \n",
    "\n",
    "\n",
    "Аналогично, при фиксированном векторе $b =(b_ i)$  определяются значения $a=(a_j)$ :\n",
    "\n",
    "$a_j = \\frac{\\sum_{i=1}^m b_i x_{ij} }{\\sum_{i =1}^m b_i ^2 } $. \n",
    "\n",
    "\n",
    "B качестве начального приближения вектора $a$ возьмем случайный вектор единичной длины, вычисляем вектор $b$, далее для этого вектора $b$ вычисляем вектор $a$ и т. д. Каждый шаг уменьшает значение $F(b, a)$ . В качестве критерия остановки используется малость относительного уменьшения значения минимизируемого функционала $F(b, a)$  за шаг итерации $(\\Delta F / F )$ или малость самого значения $F$.\n",
    "\n",
    "В результате для матрицы $X=(x_{ij})$ получили наилучшее приближение матрицей $P_1$ вида $b^1 \\otimes a^1 = (b_i^1  a_j^1)$ (здесь верхним индексом обозначен номер итерации). Далее, из матрицы $X$ вычитаем полученную матрицу $P_1$, и для полученной матрицы уклонений $X_1=X-P_1$ вновь ищем наилучшее приближение $P_2 $этого же вида и т. д., пока, например, норма $X_k$ не станет достаточно малой. В результате получили итерационную процедуру разложения матрицы X в виде суммы матриц ранга 1, то есть $X=P_1+P_2+... +P_q $ $(P_l = b^l \\otimes a^l) $ . Полагаем  $\\sigma_l = \\|a^l\\| \\|b^l\\|$ и нормируем векторы  $a^l \\, , \\, b^l: a^l:= a^l/ \\| a^l\\|; \\, \\, b^l:= b^l/ \\| b^l\\| $. В результате получена аппроксимация сингулярных чисел $ \\sigma_l$  и сингулярных векторов (правых —  $a^l$ и левых — $b^l$).\n",
    "\n",
    "К достоинствам этого алгоритма относится его исключительная простота и возможность почти без изменений перенести его на данные с пробелами, а также взвешенные данные.\n",
    "\n",
    "Существуют различные модификации базового алгоритма, улучшающие точность и устойчивость. Например, векторы главных компонент $a^l$ при разных $l$ должны быть ортогональны «по построению», однако при большом числе итерации (большая размерность, много компонент) малые отклонения от ортогональности накапливаются и может потребоваться специальная коррекция $a^l$ на каждом шаге, обеспечивающая его ортогональность ранее найденным главным компонентам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYL06Cf3LQLk"
   },
   "source": [
    "# Метод Якоби (вращений)\n",
    "\n",
    "На каждом шаге вычисляется вращение Якоби $J$, с помощью которого матрица $G^\\top G$ неявно пересчитывается в $J^\\top G^\\top GJ$; вращение выбрано так, чтобы пара внедиагональных элементов из $G^\\top G$ обратилась в нули в матрице $J^\\top G^\\top GJ$. При этом ни $G^\\top G$, ни $J^\\top G^\\top GJ$ не вычисляются в явном виде; вместо них вычисляется матрица $GJ$. Поэтому алгоритм называется методом односторонних вращений.\n",
    "\n",
    "Пусть $a_{ij}$ - элемент, расположенный в i-ой строке и j-ом столбце матрицы $G^\\top G$.\n",
    "\n",
    "$τ=(a_{jj}−a_{kk})/(2⋅a_{jk})$\n",
    "\n",
    "$t=\\frac{sign(τ)}{|\\tau| + \\sqrt{1 + \\tau^2}}$. При $τ=0$ считаем $sign(τ)=1$, то есть $θ=\\frac{\\pi}{4}$.\n",
    "\n",
    "$c=\\frac{1}{\\sqrt{1 + t^2}}$, где $c=cosθ$\n",
    "\n",
    "$s=c⋅t$, где $s=sinθ$\n",
    "\n",
    "$G=G⋅R(j,k,θ)$\n",
    "\n",
    "$J=J⋅R(j,k,θ)$\n",
    "\n",
    "Здесь $R(j,k,θ)$ — матрица вращений Якоби, которая имеет следующий вид:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Одностороннее вращение Якоби в координатной плоскости $i,j$ вычисляется в том случае, если элемент $a_{ij}$ удовлетворяет условию: $|a_{ij}| ≥ε\\sqrt{a_{ii}a_{jj}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
